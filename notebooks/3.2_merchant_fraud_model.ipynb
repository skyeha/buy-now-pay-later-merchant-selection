{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append(\"../\")\n",
    "from scripts.merchant_fraud import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/01 00:37:25 WARN Utils: Your hostname, DESKTOP-H6V94HM resolves to a loopback address: 127.0.1.1; using 192.168.0.236 instead (on interface wifi0)\n",
      "24/10/01 00:37:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/01 00:37:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"Merchant Fraud Model\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the data that we're going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/curated\"\n",
    "\n",
    "# Read in transactions dataset\n",
    "transactions = spark.read.parquet(f\"{path}/transactions.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to introduce feature that will flag a transaction with a dollar value that significantly deviates from the mean dollar value. We going to keep it simple and assume that the underlying distribution of the dollar value for all merchants' transactions is normal. if the error, difference between dollar value and average dollar value, is greater than 2 standard deviation then the transaction will be flagged as suspicious.\n",
    "\n",
    "We recognise that this may not be the best practice as the underlying transaction's dollar value distribution is normal for all merchants. Regardless, we believe this simple approach suffices for flagging transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag unusual transactions that deviate greatly from a merchant's usual dollar value\n",
    "# Calculate average and standard deviation of dollar_value per merchant\n",
    "transaction_stats = transactions.groupBy(\"merchant_abn\").agg(\n",
    "    F.avg(\"dollar_value\").alias(\"avg_dollar_value\"),\n",
    "    F.stddev(\"dollar_value\").alias(\"std_dollar_value\")\n",
    ")\n",
    "\n",
    "# Join the stats back to the original dataset\n",
    "transaction_df = transactions.join(transaction_stats, on=\"merchant_abn\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that there are certain merchant with `NULL` values for dollar value standard deviation. This is because that these merchants only have one transaction across the entirety time range of the data. Though these records seems suspcious as it seems unreasonable for a business to have only 1 transaction from Febuary 2021 to August 2022, we still going to keep it in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how many standard deviations away each transaction is, in other words, we're normalising the dollar value\n",
    "# May need extra caution to interpret this feature as it can be POSITIVE OR NEGATIVE\n",
    "transaction_df = transaction_df.withColumn(\n",
    "    \"std_diff_dollar_value\", \n",
    "    F.when(\n",
    "        F.col(\"std_dollar_value\").isNotNull() & (F.col(\"std_dollar_value\") != 0),  # Are there cases where std of dollar value is 0?\n",
    "        (F.col(\"dollar_value\") - F.col(\"avg_dollar_value\")) / F.col(\"std_dollar_value\")\n",
    "    ).otherwise(0) \n",
    ")\n",
    "transaction_df = transaction_df.drop(\"consumer_id\", \"consumer_fp\", \"name\", \"category\", \"avg_dollar_value\", \"std_dollar_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+--------------+------------------+--------------------+-----------+-------------+---------+---------------------+--------------------+------------------------+---------------------------+---------------------+\n",
      "|merchant_abn|order_month|order_datetime|      dollar_value|            order_id|merchant_fp|revenue_level|take_rate|std_diff_dollar_value|monthly_order_volume|avg_monthly_order_volume|stddev_monthly_order_volume|std_diff_order_volume|\n",
      "+------------+-----------+--------------+------------------+--------------------+-----------+-------------+---------+---------------------+--------------------+------------------------+---------------------------+---------------------+\n",
      "| 96161947306|    2021-08|    2021-08-19| 63.60772275481862|e7da0886-4c01-4f1...|       NULL|            b|     4.52|  -0.5164591992066518|                 668|       655.3333333333334|         121.39483466092345|  0.10434271525676357|\n",
      "| 92779316513|    2021-08|    2021-08-16|10.081895520137127|5065b7d1-b838-4d7...|       NULL|            c|     2.24|  -1.4292730778983764|                 471|       466.1666666666667|           80.2607881698595|  0.06022035720736152|\n",
      "| 19854089605|    2021-08|    2021-08-19|244.11185528431417|ab5d50f5-cf77-47f...|       NULL|            b|     3.15|   2.1406084184297756|                 651|       641.3888888888889|         114.16146402915936|  0.08418875136934295|\n",
      "| 27888724678|    2021-08|    2021-08-15| 278.8957491120757|97329434-96eb-40b...|       NULL|            b|     4.86|  -0.6237457720939349|                 146|      133.11111111111111|         24.270264762338027|   0.5310567896601414|\n",
      "| 33233265647|    2021-08|    2021-08-27| 40.93297316366031|6887bae1-06d6-4d7...|       NULL|            b|     3.81|  -1.0371538884425873|                 261|                   255.0|         45.961620818192365|  0.13054369913832764|\n",
      "+------------+-----------+--------------+------------------+--------------------+-----------+-------------+---------+---------------------+--------------------+------------------------+---------------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Flag unusual monthly transaction volumes that deviate from a merchant's usual monthly volume\n",
    "# Extract month and year from order_datetime\n",
    "transaction_df = transaction_df.withColumn(\"order_month\", F.date_format(F.col(\"order_datetime\"), \"yyyy-MM\"))\n",
    "\n",
    "# Calculate number of transactions per merchant per month\n",
    "transaction_records_monthly = transaction_df.groupBy(\"merchant_abn\", \"order_month\").agg(\n",
    "    F.count(\"order_id\").alias(\"monthly_order_volume\")\n",
    ")\n",
    "\n",
    "# Calculate the average and standard deviation of monthly transactions per merchant\n",
    "transaction_stats = transaction_records_monthly.groupBy(\"merchant_abn\").agg(\n",
    "    F.avg(\"monthly_order_volume\").alias(\"avg_monthly_order_volume\"),\n",
    "    F.stddev(\"monthly_order_volume\").alias(\"stddev_monthly_order_volume\")\n",
    ")\n",
    "\n",
    "# Join the monthly volume feature back with the original dataset\n",
    "transaction_records_final = transaction_df.join(transaction_records_monthly, on=[\"merchant_abn\", \"order_month\"], how=\"left\"\n",
    ")\n",
    "\n",
    "# Join the transaction statistics back to the original dataset \n",
    "transaction_records_final = transaction_records_final.join(transaction_stats, on=\"merchant_abn\", how=\"left\")\n",
    "\n",
    "# Calculate how many standard deviations away each monthly volume is\n",
    "transaction_records_final = transaction_records_final.withColumn(\n",
    "    \"std_diff_order_volume\", \n",
    "    F.when(F.col(\"stddev_monthly_order_volume\").isNotNull() & (F.col(\"stddev_monthly_order_volume\") != 0),\n",
    "           (F.col(\"monthly_order_volume\") - F.col(\"avg_monthly_order_volume\")) / F.col(\"stddev_monthly_order_volume\"))\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "transaction_records_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_records_final = transaction_records_final.drop(\"avg_monthly_order_volume\", \"stddev_monthly_order_volume\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suspect that there is an underlying relationship between month, day of the week, and if the order date is a weekend with the fraud probability of the merchants. Thus, we will create 3 features that capture this temporal effect. We will later encode these feature to feed into our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the weekday (1 = Sunday, 7 = Saturday)\n",
    "transaction_records_final = transaction_records_final.withColumn(\"weekday\", F.dayofweek(\"order_datetime\"))\n",
    "\n",
    "# Add a column to flag weekends (Saturday = 7, Sunday = 1)\n",
    "transaction_records_final = transaction_records_final.withColumn(\n",
    "    \"is_weekend\", \n",
    "    F.when((F.col(\"weekday\") == 7) | (F.col(\"weekday\") == 1), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Extract year and month from 'order_month' and create new columns\n",
    "transaction_records_final = transaction_records_final.withColumns(\n",
    "    {\"year\":  F.split(F.col(\"order_month\"), \"-\")[0].cast(\"integer\"),\n",
    "    \"month\": F.split(F.col(\"order_month\"), \"-\")[1].cast(\"integer\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data to train and test set to train and fine-tune our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = transaction_records_final.filter(F.col(\"merchant_fp\").isNotNull())\n",
    "test_data = transaction_records_final.filter(F.col(\"merchant_fp\").isNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using Random Forest Regression and Linear Regression to predict the fraud probability. We will aso perform a cross-validated grid search to find the \"better\" hyperparameters for the model.\n",
    "\n",
    "For now, we will use the default hyperparameters of the model to see how each model perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Assemble the data\n",
    "assembled_train, _ = assemble_data(train_data)\n",
    "train_set, validation_set = assembled_train.randomSplit([0.8,0.2], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on validation data = 2.4331797228693293\n",
      "R2 (Coefficient of Determination) on validation data: 0.8512226844236321\n"
     ]
    }
   ],
   "source": [
    "rfr_model = unoptimal_model(RandomForestRegressor(labelCol='merchant_fp', featuresCol='features'),\n",
    "                            train_set, validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_model = unoptimal_model(LinearRegression(labelCol=\"merchant_fp\", featuresCol=\"features\"),\n",
    "#                            train_set, validation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the LR performance is quite terrible with RMSE that's almost double than that of RFR. It's $R^2$ is only 0.08 which indicates that the model failed to explain a large portion of variation in the data. Thus, we will use RFR as our main model and perform cross-validated grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/01 00:39:09 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Parameter grid\n",
    "rfr_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(RandomForestRegressor(labelCol='merchant_fp', featuresCol='features').numTrees, [10, 20, 40]) \\\n",
    "    .addGrid(RandomForestRegressor(labelCol='merchant_fp', featuresCol='features').maxDepth, [5, 10, 12]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"merchant_fp\", predictionCol=\"prediction\")\n",
    "\n",
    "crossval = CrossValidator(estimator=RandomForestRegressor(labelCol='merchant_fp', featuresCol='features'),\n",
    "                          estimatorParamMaps=rfr_paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=2)\n",
    "\n",
    "cv_model = crossval.fit(train_set)\n",
    "cv_predictions = cv_model.transform(validation_set)\n",
    "cv_rmse = evaluator.evaluate(cv_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model RMSE on test data = 2.4331797228693293\n",
      "Best number of trees: 20\n",
      "Best max depth: 5\n",
      "Best max bins: 32\n"
     ]
    }
   ],
   "source": [
    "best_model = cv_model.bestModel\n",
    "print(f\"Best Model RMSE on test data = {cv_rmse}\")\n",
    "print(f\"Best number of trees: {best_model.getNumTrees}\") \n",
    "print(f\"Best max depth: {best_model.getMaxDepth()}\") \n",
    "print(f\"Best max bins: {best_model.getMaxBins()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rfr_better = RandomForestRegressor(labelCol='merchant_fp', featuresCol='features',\n",
    "                                         numTrees = 20, maxBins = 32, maxDepth=5)\n",
    "\n",
    "rfr_better_model = rfr_better.fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "assembled_test, assembler = assemble_data(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions on the test data and combine it with the existing probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1560:(0 + 20) / 21][Stage 1561:>(0 + 0) / 21][Stage 1563:> (0 + 0) / 3]\r"
     ]
    }
   ],
   "source": [
    "predictions = rfr_better_model.transform(assembled_test)\n",
    "predictions = predictions.select(*['merchant_abn', 'order_datetime', 'order_id', 'prediction'])\n",
    "predictions = predictions.withColumnRenamed(\"prediction\", \"merchant_fp\")\n",
    "\n",
    "train_data = train_data.select(*['merchant_abn', 'order_datetime', 'order_id', 'merchant_fp'])\n",
    "\n",
    "final_df = train_data.union(predictions)\n",
    "# predictions.write.parquet(f\"../data/curated/transactions_predicted_merchant_fp.parquet\", mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_df.write.parquet(f\"../data/curated/predicted_merchant_fp.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Feature  Importance\n",
      "7   norm_monthly_order_volume    0.314107\n",
      "2                 month_index    0.151475\n",
      "8  norm_std_diff_order_volume    0.133240\n",
      "5           norm_dollar_value    0.109563\n",
      "0               revenue_index    0.097277\n",
      "9                   take_rate    0.094642\n",
      "3               weekday_index    0.060598\n",
      "6  norm_std_diff_dollar_value    0.032338\n",
      "1                  year_index    0.004853\n",
      "4           is_weekend_vector    0.001908\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rfr_feature_importances = rfr_better_model.featureImportances\n",
    "feature_names = assembler.getInputCols()\n",
    "\n",
    "rf_importances_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance\": rfr_feature_importances.toArray()\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "print(rf_importances_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
