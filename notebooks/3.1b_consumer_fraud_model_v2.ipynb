{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append(\"../\")\n",
    "from scripts.consumer_transaction_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = \"/usr/local/bin/python3.11\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = \"/usr/local/bin/python3.11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 22:52:42 WARN Utils: Your hostname, DESKTOP-H6V94HM resolves to a loopback address: 127.0.1.1; using 192.168.0.236 instead (on interface wifi0)\n",
      "24/09/30 22:52:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/30 22:52:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark Session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"consumer transaction model\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.execturo.memory\", \"2g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Loading the necessary data\n",
    "consumer_info = spark.read.parquet('../data/curated/consumer_info.parquet')\n",
    "\n",
    "transaction_records = spark.read.parquet('../data/curated/transactions.parquet')\n",
    "transaction_records = transaction_records.drop(\"name\") # drop name so it doesn't conflict when merge with consumer\n",
    "\n",
    "fraudulent_consumer_rate = spark.read.parquet('../data/curated/consumer_fp.parquet')\n",
    "\n",
    "personal_fraud = spark.read.csv('../data/curated/personal_fraud.csv', header=True, inferSchema=True)\n",
    "postcode_info = spark.read.csv('../data/curated/postcode_info.csv', header=True, inferSchema=True)\n",
    "\n",
    "personal_fraud = personal_fraud.drop(personal_fraud.columns[0])\n",
    "postcode_info = postcode_info.drop(postcode_info.columns[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consumer Fraud Probability Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are a lot of missing fraud probability for consumers in the transactions dataset, we would like to use machine learning algorthm to predict the missing values. This version of the model predict fraud probability at a transactional level. In other words, for any transactions that the customer made, the fraud probability is compute for that transaction only and all transactions made by a customer may have different fraud probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do some feature engineering and data aggregation as we believe that the current data we have on each transactions isn't enough for us to accurately predict the fraud probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>consumer_id</th><th>order_datetime</th><th>dollar_value</th><th>order_id</th><th>consumer_fp</th><th>name</th><th>gender</th><th>state</th><th>postcode</th></tr>\n",
       "<tr><td>285333</td><td>2021-08-19</td><td>244.11185528431417</td><td>ab5d50f5-cf77-47f...</td><td>NULL</td><td>Jose Rodriguez</td><td>Undisclosed</td><td>WA</td><td>6901</td></tr>\n",
       "<tr><td>255477</td><td>2021-08-19</td><td>63.60772275481862</td><td>e7da0886-4c01-4f1...</td><td>NULL</td><td>Dawn Rush</td><td>Female</td><td>ACT</td><td>2911</td></tr>\n",
       "<tr><td>458016</td><td>2021-08-15</td><td>278.8957491120757</td><td>97329434-96eb-40b...</td><td>NULL</td><td>Heather Martinez</td><td>Female</td><td>NT</td><td>837</td></tr>\n",
       "<tr><td>471660</td><td>2021-08-16</td><td>10.081895520137127</td><td>5065b7d1-b838-4d7...</td><td>NULL</td><td>Jennifer Guzman</td><td>Female</td><td>WA</td><td>6068</td></tr>\n",
       "<tr><td>948149</td><td>2021-08-27</td><td>40.93297316366031</td><td>6887bae1-06d6-4d7...</td><td>NULL</td><td>Stephen Nguyen</td><td>Male</td><td>SA</td><td>5139</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------+--------------+------------------+--------------------+-----------+----------------+-----------+-----+--------+\n",
       "|consumer_id|order_datetime|      dollar_value|            order_id|consumer_fp|            name|     gender|state|postcode|\n",
       "+-----------+--------------+------------------+--------------------+-----------+----------------+-----------+-----+--------+\n",
       "|     285333|    2021-08-19|244.11185528431417|ab5d50f5-cf77-47f...|       NULL|  Jose Rodriguez|Undisclosed|   WA|    6901|\n",
       "|     255477|    2021-08-19| 63.60772275481862|e7da0886-4c01-4f1...|       NULL|       Dawn Rush|     Female|  ACT|    2911|\n",
       "|     458016|    2021-08-15| 278.8957491120757|97329434-96eb-40b...|       NULL|Heather Martinez|     Female|   NT|     837|\n",
       "|     471660|    2021-08-16|10.081895520137127|5065b7d1-b838-4d7...|       NULL| Jennifer Guzman|     Female|   WA|    6068|\n",
       "|     948149|    2021-08-27| 40.93297316366031|6887bae1-06d6-4d7...|       NULL|  Stephen Nguyen|       Male|   SA|    5139|\n",
       "+-----------+--------------+------------------+--------------------+-----------+----------------+-----------+-----+--------+"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add consumer info to transaction records\n",
    "transaction_fraudulent_consumer_with_info = transaction_records.join(consumer_info, on=\"consumer_id\", how=\"inner\")\n",
    "transaction_fraudulent_consumer_with_info = transaction_fraudulent_consumer_with_info.drop(\n",
    "    \"merchant_abn\", \"merchant_fp\", \"category\", \"revenue_level\", \"take_rate\"\n",
    ")\n",
    "transaction_fraudulent_consumer_with_info.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each consumer's area of living will have their associated fraud probability. The state and postcode average fraud probability will be useful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average fraud probability in each postcode or state\n",
    "consumer_info_with_fp = consumer_info.join(fraudulent_consumer_rate, on = 'consumer_id', how = 'inner')\n",
    "\n",
    "fraudulent_consumer_group_by_postcode = consumer_info_with_fp.groupBy([\"postcode\"]).agg(F.avg(\"fraud_probability\").alias(\"average_fraud_prob_of_postcode\"))\n",
    "\n",
    "fraudulent_consumer_group_by_state = consumer_info_with_fp.groupBy([\"state\"]).agg(F.avg(\"fraud_probability\").alias(\"average_fraud_prob_of_state\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe that consumer with fluctutating buying behaviour, i.e high standard deviation accross all order, are potentially commiting fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis order value, consider the variance of order value and purchase frequency\n",
    "consumer_transaction_value_analysis =  transaction_fraudulent_consumer_with_info.groupBy(\"consumer_id\", \"state\", \"postcode\") \\\n",
    "                                        .agg(\n",
    "                                            F.avg(\"dollar_value\").alias(\"average_dollar_value\"),\n",
    "                                            F.min(\"dollar_value\").alias(\"min_dollar_value\"),\n",
    "                                            F.max(\"dollar_value\").alias(\"max_dollar_value\"),\n",
    "                                            F.count(\"dollar_value\").alias(\"transaction_count\"),\n",
    "                                            F.stddev(\"dollar_value\").alias(\"stddev_dollar_value\")\n",
    "                                        )\n",
    "\n",
    "# consumer_transaction_value_analysis.limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consumer with high standard deviation in the dollar value of their transactions may be suspicious as that mean their shopping habid varies a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_fraudulent_consumer_summary = transaction_fraudulent_consumer_with_info \\\n",
    "    .join(consumer_transaction_value_analysis, on=[\"consumer_id\", \"state\",\"postcode\"], how=\"left\") \\\n",
    "    .join(fraudulent_consumer_group_by_postcode, on=\"postcode\", how=\"inner\") \\\n",
    "    .join(fraudulent_consumer_group_by_state, on=\"state\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have data on personal fraud rate and income from each postcode, we can use it to help predicting consumer fraud probability. We will also create a feature that calculate the proportion of the mean/median income of the consumer's respective location that is used for making transactions. We think that it is unreasonable for a person to spend more than 70% of their annual salary on purchasing items as that would mean they wouldn't have enough money for other neccessity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get infomation on personal fraud and income from external dataset\n",
    "postcode_info = postcode_info.drop(\"state\", \"long\", \"lat\", \"lgacode\")\n",
    "transaction_fraudulent_consumer_summary = transaction_fraudulent_consumer_summary.join(personal_fraud, on=\"state\", how=\"inner\")\n",
    "transaction_fraudulent_consumer_summary = transaction_fraudulent_consumer_summary.join(postcode_info, on=\"postcode\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get proportion of the money used to purchase item with respect to income (one and a half year)\n",
    "# average income\n",
    "transaction_fraudulent_consumer_summary = transaction_fraudulent_consumer_summary.withColumn(\"Proportion_between_max_order_value_mean_income\", F.col(\"max_dollar_value\") / (F.col(\"mean_income\") * 1.5) )\n",
    "transaction_fraudulent_consumer_summary = transaction_fraudulent_consumer_summary.withColumn(\"Proportion_between_max_order_value_median_income\", F.col(\"max_dollar_value\") / (F.col(\"median_income\") * 1.5))\n",
    "\n",
    "# Total income\n",
    "transaction_fraudulent_consumer_summary = transaction_fraudulent_consumer_summary.withColumn(\"Proportion_between_total_order_value_mean_income\", F.col(\"average_dollar_value\") * F.col(\"transaction_count\") / (F.col(\"mean_income\") * 1.5))\n",
    "transaction_fraudulent_consumer_summary = transaction_fraudulent_consumer_summary.withColumn(\"Proportion_between_total_order_value_median_income\", F.col(\"average_dollar_value\") * F.col(\"transaction_count\") / (F.col(\"median_income\") * 1.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As prediting consumer's fraud probability on a transactional level is our main goal, we suspect that there is a temporal relationship between fraud probability and the month, date of purchase. Thus, we will split the `order_datetime` column into month, day (Monday - Sunday).\n",
    "\n",
    "We also introduce a feature that indicates the number of order that a customer made in the previous week. Hence, we will train our model on 2021-03-07, which is 6 days before the first date of entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'order_datetime' from string to date format\n",
    "transaction_fraudulent_consumer_summary = transaction_fraudulent_consumer_summary.withColumn(\"order_datetime\", F.to_date(\"order_datetime\", \"yyyy-MM-dd\"))\n",
    "cutoff_date = \"2021-03-07\"\n",
    "transaction_fraudulent_consumer_summary = transaction_fraudulent_consumer_summary.filter(F.col(\"order_datetime\") >= F.lit(cutoff_date))\n",
    "\n",
    "# Add a new column 'transaction_count_last_n_days' that counts the transactions within n days before each transaction\n",
    "window_spec = Window.partitionBy(\"consumer_id\").orderBy(F.col(\"order_datetime\").cast(\"long\")) \\\n",
    "    .rangeBetween(-7 * 86400, 0)  # 7 days in seconds (86400 seconds = 1 day)\n",
    "\n",
    "transaction_fraudulent_consumer_summary = transaction_fraudulent_consumer_summary.withColumn(\"transaction_count_last_7_days\", F.count(\"order_datetime\").over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the corresponding day of the week for the given date in the DataFrame.\n",
    "transaction_fraudulent_consumer_summary = transaction_fraudulent_consumer_summary.withColumn(\"day_of_week\", F.dayofweek(\"order_datetime\"))\n",
    "transaction_fraudulent_consumer_summary = transaction_fraudulent_consumer_summary.withColumn(\"is_weekend\", F.when((F.col(\"day_of_week\") == 7) | (F.col(\"day_of_week\") == 1), 1).otherwise(0))\n",
    "\n",
    "transaction_fraudulent_consumer_summary = transaction_fraudulent_consumer_summary.withColumn(\"month\", F.month(\"order_datetime\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:=====================================================>  (20 + 1) / 21]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------------------------+\n",
      "|month|  count|average_fraud_probability|\n",
      "+-----+-------+-------------------------+\n",
      "|   12| 910143|        14.34681743793918|\n",
      "|    1| 525394|        14.65688007921009|\n",
      "|    6|1312987|        13.90154329508384|\n",
      "|    3|1051393|        14.73715900790723|\n",
      "|    5|1329977|       14.764061886055469|\n",
      "|    9| 644670|        14.60164630257746|\n",
      "|    4|1174513|       14.588978233806925|\n",
      "|    8|1365693|       14.731618998459677|\n",
      "|    7|1375524|       14.511252271710653|\n",
      "|   10| 693724|       14.602753834274727|\n",
      "|   11| 942185|       14.623580500547867|\n",
      "|    2| 504446|       14.772366739280768|\n",
      "+-----+-------+-------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "monthly_summary = transaction_fraudulent_consumer_summary.withColumn(\"month\", F.month(\"order_datetime\")) \\\n",
    "    .groupBy(\"month\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"count\"),\n",
    "        F.avg(\"consumer_fp\").alias(\"average_fraud_probability\")\n",
    "    )\n",
    "\n",
    "monthly_summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the average fraud probability of each month doesn't vary a lot. This may suggest that there is little to no temporal relationship between consumer's fraud probability and month. Though, during our model fitting, we will stil include this feature in the model and will check the feature importance.\n",
    "\n",
    "During our preliminary analysis, we found out that the distribution of the dollar value for the transactions are heavily right-skewed even after a log-transformation. Thus, we will do a log-transformation on the feature `dollar_value` as well as any other features that are related to it, and then normalise for better comparision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions or Observations:\n",
    "1. The gender plot shows that there is a similar number of male and female consumers.\n",
    "2. The number of consumers varies significantly across different states.\n",
    "3. Consumers make a similar number of purchases on each day of the week, whether itâ€™s a weekday or a weekend.\n",
    "4. Both fraud probability and the dollar value of an order are strongly right-skewed and should be normalized.\n",
    "5. Proportion features exhibit a linear relationship with fraud probability but may need transformation to clarify this relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert relevant columns to Pandas\n",
    "# df_pandas = transaction_fraudulent_consumer_summary.select(\n",
    "#     \"dollar_value\", \"scaled_dollar_value\", \"consumer_fp\", \"scaled_average_dollar_value\", \n",
    "#     \"scaled_min_dollar_value\", \"scaled_max_dollar_value\", \"transaction_count\", \n",
    "#     \"median_income\", \"mean_income\", \"state\", \"gender\", \"scaled_stddev_dollar_value\",\n",
    "#     \"day_of_week\", \"is_weekend\", \"Proportion_between_max_order_value_mean_income\",\n",
    "#     \"Proportion_between_max_order_value_median_income\", \n",
    "#     \"Proportion_between_total_order_value_mean_income\", \n",
    "#     \"Proportion_between_total_order_value_median_income\"\n",
    "# ).toPandas()\n",
    "\n",
    "# # Define plots in a dictionary for looping\n",
    "# plots = {\n",
    "#     \"Dollar Value Distribution\": (\"dollar_value\", \"hist\"),\n",
    "#     \"Scaled Dollar Value Distribution\": (\"scaled_dollar_value\", \"hist\"),\n",
    "#     \"Max Dollar Value Distribution\": (\"scaled_max_dollar_value\", \"hist\"),\n",
    "#     \"Min Dollar Value Distribution\": (\"scaled_min_dollar_value\", \"hist\"),\n",
    "#     \"Std Dollar Value Distribution\": (\"scaled_stddev_dollar_value\", \"hist\"),\n",
    "#     \"Average Dollar Value Distribution\": (\"scaled_average_dollar_value\", \"hist\"),\n",
    "#     \"Fraud Probability Distribution\": (\"consumer_fp\", \"hist\"),\n",
    "#     \"Transaction Count Distribution\": (\"transaction_count\", \"hist\"),\n",
    "#     \"Gender Count\": (\"gender\", \"count\"),\n",
    "#     \"State Count\": (\"state\", \"count\"),\n",
    "#     \"Day of Week Count\": (\"day_of_week\", \"count\"),\n",
    "#     \"Is Weekend Count\": (\"is_weekend\", \"count\"),\n",
    "#     \"Scatter 1 (Max Order Value vs Fraud Prob - Mean Income)\": (\"Proportion_between_max_order_value_mean_income\", \"scatter1\"),\n",
    "#     \"Scatter 2 (Max Order Value vs Fraud Prob - Median Income)\": (\"Proportion_between_max_order_value_median_income\", \"scatter2\"),\n",
    "#     \"Scatter 3 (Total Order Value vs Fraud Prob - Mean Income)\": (\"Proportion_between_total_order_value_mean_income\", \"scatter3\"),\n",
    "#     \"Scatter 4 (Total Order Value vs Fraud Prob - Median Income)\": (\"Proportion_between_total_order_value_median_income\", \"scatter4\")\n",
    "# }\n",
    "# feature_visualisation(df_pandas, plots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pandas = transaction_fraudulent_consumer_summary.select(\n",
    "#     \"scaled_dollar_value\", \"fraud_probability\", \"scaled_average_dollar_value\", \n",
    "#     \"scaled_min_dollar_value\", \"scaled_max_dollar_value\", \"transaction_count\", \n",
    "#     \"median_income\", \"mean_income\", \"Proportion_between_max_order_value_mean_income\",\n",
    "#     \"Proportion_between_max_order_value_median_income\", \n",
    "#     \"Proportion_between_total_order_value_mean_income\", \n",
    "#     \"Proportion_between_total_order_value_median_income\"\n",
    "# ).toPandas()\n",
    "# corr_matrix = df_pandas.corr()\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "# plt.title(\"Correlation Heatmap of Numeric Features\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea\n",
    "1. Time Frequency feature: https://ieeexplore.ieee.org/document/9399421/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "First, let's split our train data and the data that we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = transaction_fraudulent_consumer_summary.filter(F.col(\"consumer_fp\").isNotNull())\n",
    "predict_data = transaction_fraudulent_consumer_summary.filter(F.col(\"consumer_fp\").isNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using 2 regression model, one is Random Forest Regression (RFR) and the other is Linear Regression (LR). We will use LR as the baseline model to compare with RFR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of feature to be used in the model\n",
    "features_rf = [\"norm_dollar_value\", \"norm_max_dollar_value\",\"average_fraud_prob_of_postcode\", \"norm_stddev_dollar_value\", \"Proportion_between_max_order_value_median_income\",\n",
    "               \"Proportion_between_max_order_value_mean_income\", \"transaction_count_last_7_days\", \"month_index\", \"weekday_index\", \"is_weekend_vector\"]\n",
    "\n",
    "features_lr = ['norm_dollar_value', 'norm_average_dollar_value', 'norm_stddev_dollar_value', 'average_fraud_prob_of_postcode', 'Proportion_between_total_order_value_mean_income',\n",
    "               'Proportion_between_max_order_value_median_income', 'Proportion_between_max_order_value_mean_income', 'month_index', 'weekday_index', 'is_weekend_vector',\n",
    "               'transaction_count_last_7_days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "assembled_train_data_rf, rf_assembler = assemble_data(train_data, features_rf)\n",
    "assembled_train_data_lr, _ = assemble_data(train_data, features_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_rf, validate_set_rf = assembled_train_data_rf.randomSplit([0.8, 0.2], seed=123)\n",
    "train_set_lr, validate_set_lr = assembled_train_data_lr.randomSplit([0.8, 0.2], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Parameter grid\n",
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(RandomForestRegressor(labelCol='consumer_fp', featuresCol='features').numTrees, [10, 20, 40]) \\\n",
    "    .addGrid(RandomForestRegressor(labelCol='consumer_fp', featuresCol='features').maxDepth, [5, 10, 12]) \\\n",
    "    .build()\n",
    "\n",
    "rf_evaluator = RegressionEvaluator(labelCol=\"consumer_fp\", predictionCol=\"prediction\")\n",
    "\n",
    "rf_crossval = CrossValidator(estimator=RandomForestRegressor(labelCol='consumer_fp', featuresCol='features'),\n",
    "                          estimatorParamMaps=rf_paramGrid,\n",
    "                          evaluator=rf_evaluator,\n",
    "                          numFolds=2)\n",
    "\n",
    "rf_model = rf_crossval.fit(train_set_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 23:02:19 WARN Instrumentation: [a2b48f26] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/09/30 23:02:19 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/09/30 23:02:20 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "24/09/30 23:02:58 WARN Instrumentation: [6ea8a951] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/09/30 23:03:02 WARN Instrumentation: [11654c4f] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/09/30 23:03:04 WARN Instrumentation: [017340ad] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/09/30 23:03:06 WARN Instrumentation: [91c799a2] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/09/30 23:03:09 WARN Instrumentation: [32b4e5d2] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/09/30 23:03:11 WARN Instrumentation: [1db83b80] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/09/30 23:03:13 WARN Instrumentation: [db1bef7b] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/09/30 23:03:15 WARN Instrumentation: [a5d3bc9e] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/09/30 23:03:57 WARN Instrumentation: [b2eaf54d] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/09/30 23:04:33 WARN Instrumentation: [765b5b45] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/09/30 23:04:35 WARN Instrumentation: [31d58b9c] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/09/30 23:04:38 WARN Instrumentation: [559c6308] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/09/30 23:04:40 WARN Instrumentation: [c38e9cfd] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/09/30 23:04:42 WARN Instrumentation: [e1218506] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/09/30 23:04:44 WARN Instrumentation: [257f681c] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/09/30 23:04:46 WARN Instrumentation: [0d9ae017] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/09/30 23:04:49 WARN Instrumentation: [7cefe1bd] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/09/30 23:05:53 WARN Instrumentation: [9c3ebc5e] regParam is zero, which might cause numerical instability and overfitting.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(LinearRegression(labelCol=\"consumer_fp\", featuresCol=\"features\").regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(LinearRegression(labelCol=\"consumer_fp\", featuresCol=\"features\").elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "    \n",
    "lr_evaluator = RegressionEvaluator(labelCol=\"consumer_fp\", predictionCol=\"prediction\")\n",
    "\n",
    "lr_crossval = CrossValidator(estimator=LinearRegression(labelCol=\"consumer_fp\", featuresCol=\"features\"),\n",
    "                          estimatorParamMaps=lr_paramGrid,\n",
    "                          evaluator=lr_evaluator,\n",
    "                          numFolds=2)\n",
    "\n",
    "lr_model = lr_crossval.fit(train_set_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3800:============================================>         (18 + 4) / 22]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFR's Root Mean Squared Error (RMSE) on validation data = 6.811379335834735\n",
      "RFR's R2 (Coefficient of Determination) on validation data: 0.4257320912403959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rf_predictions = rf_model.transform(validate_set_rf)\n",
    "rf_rmse = rf_evaluator.evaluate(rf_predictions, {rf_evaluator.metricName: \"rmse\"})\n",
    "rf_r2 = rf_evaluator.evaluate(rf_predictions, {rf_evaluator.metricName: \"r2\"})\n",
    "print(f\"RFR's Root Mean Squared Error (RMSE) on validation data = {rf_rmse}\")\n",
    "print(f\"RFR's R2 (Coefficient of Determination) on validation data: {rf_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1699:=================================================>    (20 + 2) / 22]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR's Root Mean Squared Error (RMSE) on validation data = 7.829881311475111\n",
      "LR's R2 (Coefficient of Determination) on validation data: 0.24115204065560436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr_predictions = lr_model.transform(validate_set_lr)\n",
    "lr_rmse = lr_evaluator.evaluate(lr_predictions, {lr_evaluator.metricName: \"rmse\"})\n",
    "lr_r2 = lr_evaluator.evaluate(lr_predictions, {lr_evaluator.metricName: \"r2\"})\n",
    "print(f\"LR's Root Mean Squared Error (RMSE) on validation data = {lr_rmse}\")\n",
    "print(f\"LR's R2 (Coefficient of Determination) on validation data: {lr_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the result, we can see that the RFR model perform better than the LR model both in terms of prediction accuracy as well as variation in data explained by the model. In fact, we expected this as linear regression is a simple model and only work best with linear data. It's very unlikely the case that the relationship between the features and the response variable is linear, thus giving RFR the upper hand. Based on such performance, we will use RFR to predict our missing fraud probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the hyperparameters of the \"better\" model for RFR and its feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest numTrees: 20\n",
      "Best Random Forest maxDepth: 5\n"
     ]
    }
   ],
   "source": [
    "best_rf_model = rf_model.bestModel\n",
    "print(f\"Best Random Forest numTrees: {best_rf_model.getNumTrees}\") # 20\n",
    "print(f\"Best Random Forest maxDepth: {best_rf_model.getMaxDepth()}\") # 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Feature  Importance\n",
      "0                                 norm_dollar_value    0.425110\n",
      "1                             norm_max_dollar_value    0.205717\n",
      "2                    average_fraud_prob_of_postcode    0.109584\n",
      "3                          norm_stddev_dollar_value    0.097465\n",
      "4  Proportion_between_max_order_value_median_income    0.067603\n",
      "5    Proportion_between_max_order_value_mean_income    0.048784\n",
      "7                                       month_index    0.042556\n",
      "8                                     weekday_index    0.002601\n",
      "6                     transaction_count_last_7_days    0.000503\n",
      "9                                 is_weekend_vector    0.000076\n"
     ]
    }
   ],
   "source": [
    "feature_names_rf = rf_assembler.getInputCols()\n",
    "rf_feature_importances = rf_model.bestModel.featureImportances\n",
    "\n",
    "rf_importances_df = pd.DataFrame({\n",
    "    \"Feature\": features_rf,\n",
    "    \"Importance\": rf_feature_importances.toArray()\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "print(rf_importances_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the feature importance table, we can see that the bottom 3 have very little impact on the predictions of the fraud probabilities. Thus, we will exclude them from our model. Using the `best_rf_model` to predict our missing consumer fraud probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_features_rf = [\"norm_dollar_value\", \"norm_max_dollar_value\",\"average_fraud_prob_of_postcode\", \"norm_stddev_dollar_value\", \"Proportion_between_max_order_value_median_income\",\n",
    "               \"Proportion_between_max_order_value_mean_income\", \"month_index\"]\n",
    "\n",
    "assembled_prediction_data, _ = assemble_data(predict_data, final_features_rf)\n",
    "\n",
    "final_rf = RandomForestRegressor(labelCol='consumer_fp', featuresCol='features',\n",
    "                                 numTrees=20, maxDepth=5)\n",
    "\n",
    "final_rf_model = final_rf.fit(train_set_rf)\n",
    "predictions = final_rf_model.transform(assembled_prediction_data)\n",
    "predictions = predictions.select(*['consumer_id', 'order_datetime', 'order_id', 'prediction'])\n",
    "predictions = predictions.withColumnRenamed('prediction', 'consumer_fp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that's left is to select the same columns from the train data and concat them togther."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.select(*['consumer_id', 'order_datetime', 'order_id', 'consumer_fp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_df = train_data.union(predictions)\n",
    "final_df.write.parquet(f\"../data/curated/predicted_consumer_fp.parquet\", mode = \"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
